import concurrent.futures
import itertools
import json
import os
import time
from functools import partial
from pathlib import Path

import numpy as np
from numpy.lib.npyio import NpzFile

from dataset_util.arr_util import minmax_normalization
from target_generation.functional import flow
from target_generation.graphs import NDArrayGraph2D, NDArrayGraph2D_Inertial
from target_generation.settings import TargetGenSettings


def create_visualized_signal_arr(shape:tuple[int,int], coords:tuple[int,int], r:int=5) -> np.ndarray:
	h, w = shape
	i0, j0 = coords
	get_irange = lambda ii : range(max(0, ii - r), min(h, ii + r))
	get_jrange = lambda jj : range(max(0, jj - r), min(w, jj + r))
	arr = np.zeros(shape=(h, w))
	for i, j in itertools.product(get_irange(i0), get_jrange(j0)):
		arr[i, j] = 1
	return arr

def safe_log_map(arr:np.ndarray) -> np.ndarray:
	result = np.zeros_like(arr, dtype=np.float32)
	mask = arr == 0 # entity starting position
	result[mask] = -np.inf
	result[~mask] = np.log(arr[~mask])
	return result

def generate_imm_baseline(idx:int, signal_coords:tuple[int,int], settings:TargetGenSettings) -> tuple[int, np.ndarray, tuple[int, int]]:
	archive:NpzFile
	with np.load(settings.environments_dataset_file, allow_pickle=False) as archive:
		env = archive[str(idx)]
	
	mm = flow(source=signal_coords, graph=NDArrayGraph2D_Inertial(array=env))
	mm = minmax_normalization(mm)
	
	return idx, mm, signal_coords

def generate_smm_baseline(idx:int, signal_coords:tuple[int,int], settings:TargetGenSettings) -> tuple[int, np.ndarray, tuple[int, int]]:
	archive:NpzFile
	with np.load(settings.environments_dataset_file, allow_pickle=False) as archive:
		env = archive[str(idx)]
	
	mm = flow(source=signal_coords, graph=NDArrayGraph2D(array=env))
	mm = minmax_normalization(mm)

	return idx, mm, signal_coords

def main(motion_model_name:str, settings_filepath:str) -> None:
	# Dynamic motion model selection.
	with open(settings_filepath, 'r') as ifs:
		match (motion_model_name):
			case 'smm':
				settings = TargetGenSettings(**json.load(ifs))
				dataset_generator = generate_smm_baseline
			case 'imm':
				settings = TargetGenSettings(**json.load(ifs))
				dataset_generator = generate_imm_baseline
			case _:
				raise ValueError("Unidentified motion model found.")

	if os.path.exists(settings.motion_models_dataset_file):
		print("!!!Baselines already exist; delete existing baselines before attempting to build new ones.")
		exit(-1)

	rng = np.random.default_rng(settings.seed)
	
	# Load Environments
	archive:NpzFile
	with np.load(settings.environments_dataset_file, allow_pickle=False) as archive:
		h,w = archive[archive.files[0]].shape
		n_samples = len(archive)

	# Define Signals
	signals_exist = os.path.exists(settings.raw_signals_dataset_file) and os.path.exists(settings.signals_dataset_file)
	if signals_exist:
		# Load existing
		archive = np.load(settings.raw_signals_dataset_file)
		signals_dataset = list(map(tuple, archive[archive.files[0]].tolist()))
		assert len(signals_dataset) == n_samples # verify consistency
	else:
		# Generate new random
		# Note signals are generated by index (as opposed to stacking coordinate-arrays) to keep the values consistent independent of randomness induced by the rng.
		signals_dataset = [(rng.integers(0, h), rng.integers(0, w)) for _ in range(n_samples)]

	cpu_count = os.cpu_count()
	max_workers = min(len(signals_dataset), max(1, cpu_count - 2)) if cpu_count else 1 # leave a few workers free to be able to continue working without everything being sluggish.
	print(f"Using up to {max_workers} workers.")

	t0 = time.time()
	task = partial(dataset_generator, settings=settings)
	mms_dataset_map = { }
	signals_dataset_map = { }
	with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
		for progress, (idx, mm_arr, signal_coords) in enumerate(
			executor.map(task, range(n_samples), map(tuple, signals_dataset), chunksize=1)
		):
			mms_dataset_map[str(idx)] = safe_log_map(mm_arr)
			signal_arr = np.zeros_like(mm_arr)
			signal_arr[signal_coords] = 1
			signals_dataset_map[str(idx)] = signal_arr
			t1 = time.time()
			print(f"\rProcessed {progress+1}/{n_samples} samples ({(t1 - t0)/(progress+1):.3f}s per sample).", end='')
		print("")

	os.makedirs(Path(settings.motion_models_dataset_file).parent, exist_ok=True)
	np.savez(settings.motion_models_dataset_file, **mms_dataset_map)
	print(f"Saved log-target baseline heatmaps to '{settings.motion_models_dataset_file}'.")
	
	if not signals_exist:
		os.makedirs(Path(settings.signals_dataset_file).parent, exist_ok=True)
		np.savez(settings.signals_dataset_file, **signals_dataset_map)
		print(f"Saved encoded flow-origin signals to '{settings.signals_dataset_file}'.")

		np.savez(settings.raw_signals_dataset_file, np.stack(signals_dataset))
		print(f"Saved raw flow-origin signals to '{settings.signals_dataset_file}'.")

	#arrs = np.exp(np.stack(list(mms_dataset_map.values())))
	#print(f"Target baselines mean: {np.mean(arrs)}")
	#print(f"Target baselines min:  {np.min(arrs)}")
	#print(f"Target baselines max:  {np.max(arrs)}")

if __name__=="__main__":
	import argparse
	parser = argparse.ArgumentParser()
	parser.add_argument("--motion_model", help="Motion model type.", default='smm', choices=['smm', 'imm'])
	parser.add_argument("--settings", help="Settings filepath.")
	args = parser.parse_args()

	motion_model_name:str = args.motion_model.lower()
	settings_filepath:str = args.settings

	print(f"Running {__file__}...")
	print(f"Configuared motion model: {motion_model_name}")
	print(f"Configured settings filepath: {settings_filepath}")
	
	main(motion_model_name, settings_filepath)
